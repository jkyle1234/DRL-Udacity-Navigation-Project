{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train an agent to navigate a large rectangular world and pick up as many yellow bananas as possible while avoiding pickup up any blue bananas. The state space of the world is 37 dimensions which includes the agent's velocity and ray based perception. The action space is 4 (forward,back,left,right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With traditional Q learing it relies on fixed size environment made up of a table of states and actions SxA where S is the total number of states and A is the total number of actions.In a continuous environment this table would become too large to be usable in practice.To overcome this problem we use a DQN network (<a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\">Deep Q Network</a>). This uses a neural work to act as a Q-value function approximator where we pass the current environment observation as input and the output is the Q-value corresponding to a possible action.\n",
    "\n",
    "\n",
    "\n",
    "### Correlated data and catastrophic forgetting\n",
    "Correlated data and the need for a replay buffer. The problem with deep learning is that catastrophic forgetting. When something new is learned it tends to replace what has been previously learned rather than adding to it and this behaviour causes a correlated data problem. To overcome this we use the technique of a replay buffer. We store tuples of state,actions,reward,observations into a buffer which can be sampled out of order to break the sequence and to help prevent this data correlation.\n",
    "\n",
    "### Q-learning and continous space\n",
    "Since our space is continuous is becomes quickly untenable to use a tablular method like Q learning. To overcome that we will use a <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\">Function Approximator.</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the DQN algorithm. The algorithm uses two neural networks Qlocal and Qtarget which are identical to each other. The target network lags behind the local network.Once we hit our replay buffer size we synchronize the two networks. The lag is done so the local network is not chasing a moving target of Q values. The overall algorithm of DQN is below.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dqnalgo.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "<table border=\"3\" align=\"left\">\n",
    "    <tr>\n",
    "    <th>Hyperparmameter</th>\n",
    "    <th>Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>Replay buffer size</td>\n",
    "    <td>100,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>gamma</td>\n",
    "    <td>0.99</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>learning rate</td>\n",
    "    <td>0.02</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>hidden layer</td>\n",
    "    <td>64</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>batch size</td>\n",
    "    <td>64</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>update network</td>\n",
    "    <td>4</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results for solving the environment with a score of 13 for at least 100 consecutive episodes.\n",
    "\n",
    "<img src=\"results.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double DQN -> DQN has been shown to overestimate the actions values. Double DQN has been shown to reduce this.\n",
    "\n",
    "Prioritized Experience replay -> This method tends to emphasize more important transitions that would be sampled with a higher probability and hence make the agent more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
